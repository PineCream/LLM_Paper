# Scaling Law in Large Neural Networks

文章介绍了大模型训练过程中的 scaling laws(缩放定律），通过实验验证了模型的损失随着模型大小（非嵌入层参数量），数据量大小，计算资源的变化的幂律关系

## 文章主要结论

- **性能主要依赖于规模，弱依赖于模型形状：**  
  模型性能最主要取决于规模，包括三个因素：模型参数数量 N（不含嵌入层）、数据集大小 D、以及用于训练的计算量 C。在合理范围内，性能对其他结构超参数（如深度与宽度的比例）依赖很弱。

- **平滑的幂律关系：**  
  当不受另外两个因素限制时，性能与 N、D、C 这三个规模因子分别呈现幂律关系，这一趋势跨越了六个数量级以上。在高端没有观察到偏离这一趋势的迹象，尽管在损失趋近于零前，性能最终会趋于平稳。

- **过拟合的普适性：**  
  只要同步扩大 N 和 D，性能会可预测地提升；但如果 N 或 D 其中之一保持不变，另一个增加，则会进入收益递减区间。性能损失可由 $N^{0.74}/D$ 的比值预测，也就是说，每当模型规模增加8倍，只需将数据量增加约5倍即可避免性能损失。

- **训练过程的普适性：**  
  训练曲线遵循可预测的幂律，其参数与模型规模基本无关。通过外推训练曲线的前半段，可以大致预测如果训练更久会达到的损失水平。

- **迁移能力随测试性能提升：**  
  当我们在与训练分布不同的文本上评估模型时，结果与训练验证集上的表现高度相关，损失有一个大致恒定的偏移量——也就是说，迁移到新分布会带来一个固定的损失惩罚，但整体提升趋势与训练集一致。

- **样本效率：**  
  大模型比小模型更具样本效率，能用更少的优化步数和更少的数据点达到同等性能。

- **收敛效率低下：**  
  在固定计算预算 C 下，如果对模型规模 N 和可用数据 D 没有限制，最优性能是在训练非常大的模型并在远未收敛时提前停止。最优的计算效率训练比用小模型训练到收敛要高得多，数据需求随 $D \sim C^{0.27}$ 增长非常缓慢。

- **最优批量大小：**  
  这些模型的理想批量大小大致只与损失的幂有关，并可通过测量梯度噪声尺度确定；对于我们能训练的最大模型，收敛时的批量大小约为100万到200万个 token。
